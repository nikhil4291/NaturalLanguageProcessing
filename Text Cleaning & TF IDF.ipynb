{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc5d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "633132f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: '()'\n",
      "C:\\Users\\nikhilk3\n"
     ]
    }
   ],
   "source": [
    "cd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90bddc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(os.getcwd()+\"/Spark-Course-Description.txt\",'r') as fh:\n",
    "    filedata=fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a4e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data -  In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies\n"
     ]
    }
   ],
   "source": [
    "print(\"Data - \",filedata[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7861aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikhilk3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a487ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "corpus=PlaintextCorpusReader(os.getcwd(),\"Spark-Course-Description.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf359305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies. In this course, discover how to build big data pipelines around Apache Spark. Join Kumaran Ponnambalam as he takes you through how to make Apache Spark work with other big data technologies. He covers the basics of Apache Kafka Connect and how to integrate it with Spark for real-time streaming. In addition, he demonstrates how to use the various technologies to construct an end-to-end project that solves a real-world business problem.\n"
     ]
    }
   ],
   "source": [
    "print(corpus.raw())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d64d3a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in this corpus:  ['Spark-Course-Description.txt']\n"
     ]
    }
   ],
   "source": [
    "print(\"Files in this corpus: \",corpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4832a60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and', 'data', '-', 'science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', '.'], ['In', 'this', 'course', ',', 'discover', 'how', 'to', 'build', 'big', 'data', 'pipelines', 'around', 'Apache', 'Spark', '.'], ['Join', 'Kumaran', 'Ponnambalam', 'as', 'he', 'takes', 'you', 'through', 'how', 'to', 'make', 'Apache', 'Spark', 'work', 'with', 'other', 'big', 'data', 'technologies', '.'], ['He', 'covers', 'the', 'basics', 'of', 'Apache', 'Kafka', 'Connect', 'and', 'how', 'to', 'integrate', 'it', 'with', 'Spark', 'for', 'real', '-', 'time', 'streaming', '.'], ['In', 'addition', ',', 'he', 'demonstrates', 'how', 'to', 'use', 'the', 'various', 'technologies', 'to', 'construct', 'an', 'end', '-', 'to', '-', 'end', 'project', 'that', 'solves', 'a', 'real', '-', 'world', 'business', 'problem', '.']]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract paragraphs\n",
    "corpus.paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1035f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract paragraphs\n",
    "len(corpus.paras())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfcc7ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and', 'data', '-', 'science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', '.'], ['In', 'this', 'course', ',', 'discover', 'how', 'to', 'build', 'big', 'data', 'pipelines', 'around', 'Apache', 'Spark', '.'], ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract Sentences from the paragraph\n",
    "corpus.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "183b2ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b381604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and', 'data', '-', 'science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', '.']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e904d38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'course', ',', 'discover', 'how', 'to', 'build', 'big', 'data', 'pipelines', 'around', 'Apache', 'Spark', '.']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.sents()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a44f229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'order', 'to', 'construct', 'data', 'pipelines', ...]\n"
     ]
    }
   ],
   "source": [
    "print(corpus.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89528392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total Paragraphs in this corpus:  1\n",
      "\n",
      " Total Sentences in this corpus:  5\n",
      "\n",
      " First Sentences in this corpus:  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and', 'data', '-', 'science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', '.']\n",
      "\n",
      " Total words in this corpus:  120\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Total Paragraphs in this corpus: \", len(corpus.paras()))\n",
    "print(\"\\n Total Sentences in this corpus: \", len(corpus.sents()))\n",
    "print(\"\\n First Sentences in this corpus: \", corpus.sents()[0])\n",
    "print(\"\\n Total words in this corpus: \", len(corpus.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080e766",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cc3860f",
   "metadata": {},
   "source": [
    "#### Analysing Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d48a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency distribution method\n",
    "\n",
    "corp_freq=nltk.FreqDist(corpus.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1123b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently used 10 words  [('to', 8), ('data', 7), (',', 5), ('-', 5), ('how', 5), ('.', 5), ('and', 4), ('In', 3), ('big', 3), ('technologies', 3)]\n"
     ]
    }
   ],
   "source": [
    "#print most commonly word in corpus\n",
    "print(\"Most frequently used 10 words \",corp_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49b29163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#find distribution for a specific word\n",
    "print(corp_freq.get(\"Spark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d77ea5",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad386d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list=nltk.word_tokenize(corpus.raw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e615a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list:  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and']\n",
      "\n",
      " Total tokens :  110\n"
     ]
    }
   ],
   "source": [
    "print(\"Token list: \",token_list[:20])\n",
    "print(\"\\n Total tokens : \", len(token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f51187",
   "metadata": {},
   "source": [
    "Cleansing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd745e81",
   "metadata": {},
   "source": [
    "###### remove puncuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9425ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list2=list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct,token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "51622493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after removing puncuations - ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'DevOps', 'specialists']\n",
      "\n",
      " total tokens after removing puncuations - 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Token list after removing puncuations -\",token_list2[:20])\n",
    "print(\"\\n total tokens after removing puncuations -\", len (token_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152778c3",
   "metadata": {},
   "source": [
    "###### convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc2ce762",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list3=[word.lower() for word in token_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d0e8168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list lower case - ['in', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'devops', 'specialists']\n",
      "\n",
      " total tokens - 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Token list lower case -\",token_list3[:20])\n",
    "print(\"\\n total tokens -\", len (token_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e31852",
   "metadata": {},
   "source": [
    "###### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b3bf98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhilk3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a33b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc2e1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list4=list(filter(lambda token: token not in stopwords.words('english'),token_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42d86edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after removing stopwords - ['order', 'construct', 'data', 'pipelines', 'networks', 'stream', 'process', 'store', 'data', 'data', 'engineers', 'data-science', 'devops', 'specialists', 'must', 'understand', 'combine', 'multiple', 'big', 'data']\n",
      "\n",
      " total tokens - 62\n"
     ]
    }
   ],
   "source": [
    "print(\"Token list after removing stopwords -\",token_list4[:20])\n",
    "print(\"\\n total tokens -\", len (token_list4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8d01c",
   "metadata": {},
   "source": [
    "###### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3960663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3f70380",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list5=[stemmer.stem(word) for word in token_list4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b00c532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after stemming - ['order', 'construct', 'data', 'pipelin', 'network', 'stream', 'process', 'store', 'data', 'data', 'engin', 'data-sci', 'devop', 'specialist', 'must', 'understand', 'combin', 'multipl', 'big', 'data']\n",
      "\n",
      " total tokens - 62\n"
     ]
    }
   ],
   "source": [
    "print(\"Token list after stemming -\",token_list5[:20])\n",
    "print(\"\\n total tokens -\", len (token_list5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ae2d8",
   "metadata": {},
   "source": [
    "###### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03c3ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikhilk3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bc13e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9980a3ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nikhilk3/nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Local\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Local\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Local\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nikhilk3/nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Local\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Local\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Local\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m token_list6 \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken_list4\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken list after Lemmatization : \u001b[39m\u001b[38;5;124m\"\u001b[39m, token_list6[:\u001b[38;5;241m20\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal tokens after Lemmatization : \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token_list6))\n",
      "Cell \u001b[1;32mIn[115], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m token_list6 \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m token_list4 ]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken list after Lemmatization : \u001b[39m\u001b[38;5;124m\"\u001b[39m, token_list6[:\u001b[38;5;241m20\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal tokens after Lemmatization : \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token_list6))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovenances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momw_prov\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;66;03m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m provdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1284\u001b[0m provdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1285\u001b[0m fileids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_omw_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileids\u001b[49m()\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[0;32m   1287\u001b[0m     prov, langfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(fileid)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nikhilk3/nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Local\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Local\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Local\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nikhilk3\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4 ]\n",
    "print(\"Token list after Lemmatization : \", token_list6[:20])\n",
    "print(\"\\nTotal tokens after Lemmatization : \", len(token_list6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a47ce6",
   "metadata": {},
   "source": [
    "###### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ff3cfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngramsams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10d01150",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams=ngrams(token_list5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "45f88b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common bigrams: \n",
      "[(('big', 'data'), 3), (('data', 'pipelin'), 2), (('data', 'technolog'), 2), (('apach', 'spark'), 2), (('order', 'construct'), 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Most common bigrams: \")\n",
    "print(Counter(bigrams).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c905cda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common trigrams: \n",
      "[(('big', 'data', 'technolog'), 2), (('order', 'construct', 'data'), 1), (('construct', 'data', 'pipelin'), 1), (('data', 'pipelin', 'network'), 1), (('pipelin', 'network', 'stream'), 1)]\n"
     ]
    }
   ],
   "source": [
    "trigrams=ngrams(token_list5,3)\n",
    "print(\"Most common trigrams: \")\n",
    "print(Counter(trigrams).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c80e545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47b9cf",
   "metadata": {},
   "source": [
    "###### parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "86e4347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nikhilk3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5fd99a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order', 'NN'),\n",
       " ('construct', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('pipelines', 'NNS'),\n",
       " ('networks', 'NNS'),\n",
       " ('stream', 'VBP'),\n",
       " ('process', 'NN'),\n",
       " ('store', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('data', 'NNS')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(token_list4)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ec4b5",
   "metadata": {},
   "source": [
    "###### term frequency - in document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdfd9e6",
   "metadata": {},
   "source": [
    "#TF-IDF\n",
    "Text-mining technique called term frequency-inverse document frequency, or TF-IDF. A number of machine learning algorithms do not work on text values. They only work on numeric features. This means text needs to be converted to an equivalent numeric representation to do machine learning. TF-IDF is a technique to convert text to a numeric table representation. TF-IDF outputs a table. In this table, each row represents a document in the corpus. Each column represents a word in the corpus. Each cell in the table provides a value that indicates the relative strength of the word with respect to the document. A higher value indicates higher correlation between the word and the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "089672c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "17fde813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector corpus\n",
    "\n",
    "vector_corpus = [\n",
    "    'NBA is a basketball league',\n",
    "    'Basketball is popular in America',\n",
    "    'TV in America telecast basketball'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dd168b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= TfidfVectorizer(stop_words ='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc5e1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=vectorizer.fit_transform(vector_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "95d5ecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens used as features are - \n",
      "\n",
      "  ['america' 'basketball' 'league' 'nba' 'popular' 'telecast' 'tv']\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens used as features are - \")\n",
    "print(\"\\n \",vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e3ac8f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f045f318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Actual TFIDF array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.38537163, 0.65249088, 0.65249088, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.54783215, 0.42544054, 0.        , 0.        , 0.72033345,\n",
       "        0.        , 0.        ],\n",
       "       [0.44451431, 0.34520502, 0.        , 0.        , 0.        ,\n",
       "        0.5844829 , 0.5844829 ]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n Actual TFIDF array\")\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fcf5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
